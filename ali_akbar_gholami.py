# -*- coding: utf-8 -*-
"""Ali Akbar Gholami.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1da2EfvovUEL_T9snyL-JR8KLQID1b_Sr
"""
'''
!pip install keras
!pip install nltk
!pip install hazm
!pip install stopwords_guilannlp
!pip install -U -q PyDrive
'''
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from keras.callbacks import EarlyStopping
from keras.layers import Dropout
from sklearn.metrics import classification_report, confusion_matrix
from hazm import *
from stopwords_guilannlp import *
from nltk.tokenize import RegexpTokenizer
import numpy as np
import math
import re
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sb
from mpl_toolkits.axes_grid1 import make_axes_locatable
from copy import deepcopy
from string import punctuation
import random
    
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

#train_set = pd.read_csv('trainset.csv')
#valid_set = pd.read_csv('validationset.csv')

dataa = pd.read_excel('/content/DigiKala_Record Ali Akbar Gholami.xlsx')
dataa=dataa[['comment','Label']]
l=int(len(dataa)*0.8)
train_set=dataa[:l]
valid_set = dataa[l:]

valid_set

import nltk
nltk.download('punkt')

print(train_set.head(10))

train_len = train_set.shape[0]
valid_len = valid_set.shape[0]
print('Train set has {} rows.'.format(train_len))
print('Validation set has {} rows.'.format(valid_len))

cat1 = train_set["Label"]



#Test set = Valid set
test_cat1 = valid_set["Label"]



unique_cat1 = np.unique(cat1)

#in the following lines of code we print how many posts are in each category
cat1_posts_count = dict.fromkeys(unique_cat1, 0)
for cat in cat1:
  cat1_posts_count[cat] += 1

for cat, cnt in cat1_posts_count.items():
  print('Category "{}" contains {} posts.'.format(cat, cnt))

#for faster result, we define the following lines of code outside the 'preprocessing' function
hazm.normalizer = Normalizer()
stopwords = stopwords_output("Persian", "nar")
hazm.lemmatizer = Lemmatizer()
stemmer = Stemmer()


def preprocessing(text):
    
    text = re.sub('<[^<]+?>','', text)
    text = ''.join(c for c in text if not c.isdigit())
    text = ''.join(c for c in text if c not in punctuation)
    text = normalizer.normalize(text)
    text = ' '.join(word for word in text.split() if word not in stopwords) # remove stopwors from text
    return text

column1 = "comment"

dataset = train_set.append(valid_set)
dataset[column1] = dataset[column1].apply(preprocessing)
num_classes = len(unique_cat1)



# The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 50000
# Max number of words in each post.
MAX_SEQUENCE_LENGTH = 250
# The embedding dimension
EMBEDDING_DIM = 100

tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(dataset[column1].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
# print(word_index)

X = tokenizer.texts_to_sequences(dataset[column1].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

rows_of_train = len(train_set)
rows_of_test = len(valid_set)

Y = pd.get_dummies(dataset['Label']).values
print('Shape of label tensor:', Y.shape)

# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.4, random_state = 42)

X_train = X[0:rows_of_train]
X_test = X[rows_of_train: rows_of_train + rows_of_test]

Y_train = Y[0:rows_of_train]
Y_test = Y[rows_of_train:rows_of_train + rows_of_test]

print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(10, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

epochs = 5
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

#X_test.reshape

Y_pred = model.predict(X_test)

new_Y_pred = np.argmax(Y_pred, axis=1)
new_Y_test = np.argmax(Y_test, axis=1)

print(classification_report(new_Y_test, new_Y_pred))

mpl.style.use('seaborn')

conf_arr = np.zeros((num_classes, num_classes))

for i in range(len(new_Y_pred)):
        conf_arr[new_Y_pred[i]][new_Y_test[i]] += 1

print(conf_arr)

summ = conf_arr.sum()

df_cm = pd.DataFrame(conf_arr, 
  index = unique_cat1,
  columns = unique_cat1)

fig = plt.figure()

plt.clf()

ax = fig.add_subplot(111)
ax.set_aspect(1)

cmap = sb.cubehelix_palette(light=1, as_cmap=True)

res = sb.heatmap(df_cm, annot=True, vmin=0.0, vmax=np.max(conf_arr), fmt='.2f', cmap=cmap)

res.invert_yaxis()

plt.yticks([0.5,1.5,2.5,3.5,4.5,5.5], unique_cat1,va='center')

print('\n\n')

plt.title('Confusion Matrix')

plt.savefig('confusion_matrix.png', dpi=700, bbox_inches='tight' )

plt.show()

# Compute the accuracy of training data and validation data

corrects = 0
for i in range(len(new_Y_pred)):
    if int(new_Y_pred[i]) is int(new_Y_test[i]):
        corrects += 1
        
accuracy = float(corrects / len(new_Y_pred))*100
print('Accuracy (using "{}" column): {} %'.format (column1, accuracy))

accr = model.evaluate(X_test,Y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

#Similar to the binary case, we can define precision for each of the classes
precision = []
#we append the precision for each class to the variable 'precision'
for i in range(num_classes):
  precision.append(conf_arr[i][i] / np.sum(conf_arr[i], axis = 0))

print('Precisions per classes are : {}'.format(precision))

macro_precision = 100 * np.sum(precision) / num_classes

print('Macro averaged Precision is : {} %'.format(macro_precision))

weighted_precision = 0
total = 0
for i in range(num_classes):
  weighted_precision += np.sum(conf_arr, axis = 0)[i] * precision[i]
  total += np.sum(conf_arr, axis = 0)[i]

weighted_precision = 100 * weighted_precision / total
print('Weighted Precision is : {} %'.format(weighted_precision))

#Similar to the binary case, we can define recall for each of the classes
recall = []
#We append the recall for each class to the variable 'recall'
for i in range(num_classes):
  recall.append(conf_arr[i][i] / np.sum(conf_arr, axis = 0)[i])
  
print('Recalls per classes are : {}'.format(recall))

macro_recall = 100 * np.sum(recall) / num_classes
print('Macro averaged Recall is : {} %'.format(macro_recall))

weighted_recall = 0
for i in range(num_classes):
  weighted_recall += np.sum(conf_arr, axis = 0)[i] * recall[i]

weighted_recall = 100 * weighted_recall / total
print('Weighted Recall is : {} %'.format(weighted_recall))

#F1-score is a function of precision and recall
#we can now compute the per-class F1-score
f1_score = []
for i in range(num_classes):
  f1_score.append( 2*( (recall[i]*precision[i]) / (recall[i]+precision[i])))

print('F1-scores per classes are : {}'.format(f1_score))

macro_f1 = 100 * np.sum(f1_score) / num_classes

print('Macro averaged F1-score is : {} %'.format(macro_f1))

weighted_f1 = 0
for i in range(num_classes):
  weighted_f1 += np.sum(conf_arr, axis = 0)[i] * f1_score[i]

weighted_f1 = 100 * weighted_f1 / total
print('Weighted F1-score is : {} %'.format(weighted_f1))